#Code Mapping - Vector Storage init
import pinecone

PINECONE_API_KEY = "YOUR_PINECONE_API_KEY"
INDEX_NAME = "your-index-name"  # The RAG index you already built

# Connect to Pinecone
pinecone.init(
    api_key=PINECONE_API_KEY,
    environment="gcp-starter"    # or your Pinecone environment
)

# Get a handle to the existing index
index = pinecone.Index(INDEX_NAME)
#------------------------------------------------------------------------------------------------------------------
#Retrieval Agent ‚Äì finding relevant knowledge
from openai import OpenAI

OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"
EMBED_MODEL = "text-embedding-3-small"  # Or 3-large

client = OpenAI(api_key=OPENAI_API_KEY)

def get_context_from_rag(query: str, top_k: int = 5) -> str:
    """Retrieve relevant context (text chunks) from the vector database."""
    # 1. Turn query text into an embedding
    query_embed = client.embeddings.create(
        input=[query],
        model=EMBED_MODEL
    ).data[0].embedding

    # 2. Ask the vector DB for the k most similar chunks
    search_results = index.query(
        query_embed,
        top_k=top_k,
        include_metadata=True
    )

    # 3. Extract the text of each matched chunk
    contexts = [item['metadata']['text'] for item in search_results['matches']]

    # 4. Join them into one big context string with separators
    return "\n\n---\n\n".join(contexts)

#--------------------------------------------------------------------------------------------------------------
#Response Generation ‚Äì building the ‚Äúaugmented prompt‚Äù
#We intigrate both the user query and the retrival data from the RAG
def run_model(model_name: str, query: str, context: str) -> str:
    """Run a query with a given model (GPT or LLaMA) using the retrieved context."""
    # Build the augmented prompt
    augmented_query = (
        f"Context:\n{context}\n\n"
        "-----\n\n"
        f"Question:\n{query}\n\n"
        "Answer:"
    )
#--------------------------------------------------------------------------------------------------------------
#LLM Integration ‚Äì unified interface for all models
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

LLAMA_MODEL = "meta-llama/Llama-3.1-70b-instruct"

def run_model(model_name: str, query: str, context: str) -> str:
    """Run a query with a given model (GPT or LLaMA) and return its response."""
    # 1. Build augmented prompt
    augmented_query = (
        f"Context:\n{context}\n\n"
        "-----\n\n"
        f"Question:\n{query}\n\n"
        "Answer:"
    )

    # 2. OpenAI GPT models
    if model_name in ["gpt-4o", "gpt-3.5-turbo"]:
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are a bioinformatics assistant. "
                        "Use only the provided context. If you don't know, say you don't know. "
                        "Provide PubMed IDs / KEGG IDs as references when possible."
                    ),
                },
                {"role": "user", "content": augmented_query},
            ],
        )
        return response.choices[0].message.content

    # 3. Local LLaMA model
    elif model_name.lower().startswith("llama"):
        tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL)
        model = AutoModelForCausalLM.from_pretrained(
            LLAMA_MODEL,
            device_map="auto",
            torch_dtype=torch.float16
        )

        inputs = tokenizer(augmented_query, return_tensors="pt").to("cuda")
        outputs = model.generate(**inputs, max_new_tokens=500)
        return tokenizer.decode(outputs[0], skip_special_tokens=True)

    else:
        raise ValueError(f"Unsupported model: {model_name}")

#---------------------------------------------------------------------------------------------------------------
#Full Pipeline per question
def query_rag_with_model(query: str, model_name: str) -> str:
    """Full RAG ‚Üí LLM pipeline for a given model."""
    print(f"\nüîç Running query on model: {model_name}")
    
    # 1. Retrieve context from RAG
    context = get_context_from_rag(query)
    
    # 2. Run model with context
    answer = run_model(model_name, query, context)
    
    print(f"\nüß† Answer from {model_name}:\n{answer}\n")
    return answer

#---------------------------------------------------------------------------------------------------------------
#The main function
if __name__ == "__main__":
    question = "Which bacterial species are involved in butyrate production in the human gut?"

    models_to_test = ["gpt-3.5-turbo", "gpt-4o", "llama"]

    results = {}
    for model in models_to_test:
        results[model] = query_rag_with_model(question, model)


